{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Text_Generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsk2JzVDJpSh7EYdT8hDzQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sandeephm/GoogleColab-codes/blob/master/NLP_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smzoMWP_58dQ",
        "colab_type": "text"
      },
      "source": [
        "# Text generation using Kaggle Song Lyrics dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6vHiytj53zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jax-t8vR6gpT",
        "colab_type": "text"
      },
      "source": [
        "**Get the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjzdYhRG6jV3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "c2438ddd-48c8-4543-f63b-18acae0a8393"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-11 05:46:20--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.212.113, 172.217.212.138, 172.217.212.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.212.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6fstfpl7qusmsvqr2p4eipi03ksi0uci/1591854375000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-06-11 05:46:23--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6fstfpl7qusmsvqr2p4eipi03ksi0uci/1591854375000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 173.194.196.132, 2607:f8b0:4001:c1a::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|173.194.196.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv       [  <=>               ]  69.08M   173MB/s    in 0.4s    \n",
            "\n",
            "2020-06-11 05:46:24 (173 MB/s) - ‘/tmp/songdata.csv’ saved [72436445]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khkMTrLu6ngN",
        "colab_type": "text"
      },
      "source": [
        "**Pre processing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh7oyLhB6nIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e57QESkh6uZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "23c09567-b78f-4ac8-dbd4-ebb3ab46eff1"
      },
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlpYiXKD6xnV",
        "colab_type": "text"
      },
      "source": [
        "Create Sequence and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeQSt5JG64fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuEDry_368So",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "8e067381-09a6-4683-b184-222ed750d750"
      },
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa90fddx7DTs",
        "colab_type": "text"
      },
      "source": [
        "**Train the Text Generation Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqB5S5KI7C_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0ba5927-f8ae-4eba-8de8-7780b5bf5b8a"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.9885 - accuracy: 0.0272\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.4387 - accuracy: 0.0373\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.3724 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.3161 - accuracy: 0.0404\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 5.2489 - accuracy: 0.0439\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.1803 - accuracy: 0.0404\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.1220 - accuracy: 0.0434\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.0578 - accuracy: 0.0489\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.9865 - accuracy: 0.0570\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.9095 - accuracy: 0.0595\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.8243 - accuracy: 0.0666\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.7428 - accuracy: 0.0721\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.6599 - accuracy: 0.0721\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.5774 - accuracy: 0.0792\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.4949 - accuracy: 0.0848\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.4109 - accuracy: 0.0984\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.3331 - accuracy: 0.1110\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.2611 - accuracy: 0.1216\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.1964 - accuracy: 0.1352\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.1331 - accuracy: 0.1559\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.0599 - accuracy: 0.1604\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.9914 - accuracy: 0.1806\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.9292 - accuracy: 0.1821\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.8694 - accuracy: 0.1862\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.8222 - accuracy: 0.1988\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.7653 - accuracy: 0.2104\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.6761 - accuracy: 0.2250\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.6056 - accuracy: 0.2386\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.5472 - accuracy: 0.2578\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.5028 - accuracy: 0.2689\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.4361 - accuracy: 0.2780\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.3938 - accuracy: 0.2836\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.3331 - accuracy: 0.2972\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.2797 - accuracy: 0.3073\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.2453 - accuracy: 0.3163\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.1613 - accuracy: 0.3522\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.1111 - accuracy: 0.3557\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.0583 - accuracy: 0.3643\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.0006 - accuracy: 0.3819\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.9509 - accuracy: 0.3986\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.8985 - accuracy: 0.3981\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.8622 - accuracy: 0.4026\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.8174 - accuracy: 0.4147\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.7583 - accuracy: 0.4238\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.7093 - accuracy: 0.4319\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.6680 - accuracy: 0.4390\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.6234 - accuracy: 0.4390\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.5871 - accuracy: 0.4480\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.5563 - accuracy: 0.4531\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.5075 - accuracy: 0.4612\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.4699 - accuracy: 0.4672\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.4350 - accuracy: 0.4702\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.3802 - accuracy: 0.4874\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.3394 - accuracy: 0.4955\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.3264 - accuracy: 0.4960\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.2799 - accuracy: 0.5040\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.2357 - accuracy: 0.5086\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.3306 - accuracy: 0.4899\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.2532 - accuracy: 0.5015\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.1918 - accuracy: 0.5197\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.1564 - accuracy: 0.5313\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.1149 - accuracy: 0.5419\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.0817 - accuracy: 0.5489\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.0349 - accuracy: 0.5621\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.0016 - accuracy: 0.5762\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.9745 - accuracy: 0.5767\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.9567 - accuracy: 0.5732\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.9162 - accuracy: 0.5848\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.8780 - accuracy: 0.5938\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.8468 - accuracy: 0.5959\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.8230 - accuracy: 0.6075\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.7946 - accuracy: 0.6060\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.7823 - accuracy: 0.6191\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.7517 - accuracy: 0.6297\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.7197 - accuracy: 0.6211\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6908 - accuracy: 0.6337\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6765 - accuracy: 0.6453\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6459 - accuracy: 0.6483\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.6345 - accuracy: 0.6468\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.5948 - accuracy: 0.6635\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.5750 - accuracy: 0.6615\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.5439 - accuracy: 0.6726\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.5399 - accuracy: 0.6700\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.5302 - accuracy: 0.6625\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.5035 - accuracy: 0.6726\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.4753 - accuracy: 0.6862\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.4490 - accuracy: 0.6872\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.4325 - accuracy: 0.6922\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.3993 - accuracy: 0.7038\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.3810 - accuracy: 0.7139\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.3702 - accuracy: 0.7114\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.3594 - accuracy: 0.7053\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.3270 - accuracy: 0.7215\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.3096 - accuracy: 0.7245\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2933 - accuracy: 0.7291\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2688 - accuracy: 0.7346\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2524 - accuracy: 0.7346\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2431 - accuracy: 0.7397\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2297 - accuracy: 0.7407\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.2108 - accuracy: 0.7422\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1977 - accuracy: 0.7397\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1841 - accuracy: 0.7457\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1716 - accuracy: 0.7472\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1483 - accuracy: 0.7553\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1373 - accuracy: 0.7518\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1397 - accuracy: 0.7573\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1140 - accuracy: 0.7619\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0971 - accuracy: 0.7664\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0860 - accuracy: 0.7689\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0692 - accuracy: 0.7735\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0775 - accuracy: 0.7699\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0528 - accuracy: 0.7780\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0510 - accuracy: 0.7709\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0296 - accuracy: 0.7805\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0440 - accuracy: 0.7694\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0284 - accuracy: 0.7790\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0084 - accuracy: 0.7815\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9879 - accuracy: 0.7896\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9759 - accuracy: 0.7881\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9563 - accuracy: 0.7962\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9539 - accuracy: 0.7997\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9373 - accuracy: 0.8032\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9296 - accuracy: 0.8052\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9210 - accuracy: 0.8068\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9002 - accuracy: 0.8113\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8942 - accuracy: 0.8103\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8817 - accuracy: 0.8184\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8681 - accuracy: 0.8184\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8607 - accuracy: 0.8239\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8481 - accuracy: 0.8239\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8349 - accuracy: 0.8229\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8221 - accuracy: 0.8285\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8127 - accuracy: 0.8320\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8078 - accuracy: 0.8355\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.8015 - accuracy: 0.8285\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7899 - accuracy: 0.8360\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7874 - accuracy: 0.8365\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7872 - accuracy: 0.8340\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7858 - accuracy: 0.8310\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7760 - accuracy: 0.8426\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7832 - accuracy: 0.8305\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7908 - accuracy: 0.8234\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7576 - accuracy: 0.8375\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7435 - accuracy: 0.8441\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7330 - accuracy: 0.8451\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7240 - accuracy: 0.8466\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.7084 - accuracy: 0.8471\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6990 - accuracy: 0.8517\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6917 - accuracy: 0.8486\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6843 - accuracy: 0.8547\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6790 - accuracy: 0.8577\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6673 - accuracy: 0.8618\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6640 - accuracy: 0.8547\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6639 - accuracy: 0.8496\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6620 - accuracy: 0.8532\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6564 - accuracy: 0.8542\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6460 - accuracy: 0.8537\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6344 - accuracy: 0.8587\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6604 - accuracy: 0.8512\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6531 - accuracy: 0.8466\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6407 - accuracy: 0.8527\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6322 - accuracy: 0.8532\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6118 - accuracy: 0.8663\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6626 - accuracy: 0.8421\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6300 - accuracy: 0.8547\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6230 - accuracy: 0.8527\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6134 - accuracy: 0.8592\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5992 - accuracy: 0.8633\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5877 - accuracy: 0.8658\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5746 - accuracy: 0.8668\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5704 - accuracy: 0.8708\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6547 - accuracy: 0.8340\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6417 - accuracy: 0.8421\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.6034 - accuracy: 0.8552\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5774 - accuracy: 0.8602\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5571 - accuracy: 0.8713\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5458 - accuracy: 0.8683\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5396 - accuracy: 0.8713\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5294 - accuracy: 0.8759\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5240 - accuracy: 0.8764\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5176 - accuracy: 0.8759\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5145 - accuracy: 0.8739\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5086 - accuracy: 0.8799\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5027 - accuracy: 0.8744\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4985 - accuracy: 0.8809\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4950 - accuracy: 0.8784\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4921 - accuracy: 0.8789\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5023 - accuracy: 0.8749\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5427 - accuracy: 0.8663\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.5219 - accuracy: 0.8703\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4936 - accuracy: 0.8809\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4843 - accuracy: 0.8794\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4777 - accuracy: 0.8799\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4711 - accuracy: 0.8789\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4673 - accuracy: 0.8819\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4623 - accuracy: 0.8829\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4561 - accuracy: 0.8814\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4518 - accuracy: 0.8895\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4481 - accuracy: 0.8809\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.4443 - accuracy: 0.8845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC6G2l328EIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "c0f07918-4372-45ae-e5f9-34298888c7e3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVeyMlHK8HUB",
        "colab_type": "text"
      },
      "source": [
        "**Generate New Lyrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcQFUknQ8G5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1314e73d-d747-44a0-ef0f-9ce2f998033b"
      },
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "im feeling chills me to the bone tender another break rain rain rely learn am life had kind of friend im blue life to friend hardly wanted none on away andante make out me mad friend and i do out could away i think youll learn to so slowly with on past past ways ways feeling be ways shouting a be in every little advice to be come good care good care ways gone for again and even better out that could good care for joe joe ways ways knew kind away cassandra yourself a your bedumbedumdum bedumbedumdum bedumbedumdum so start park park\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}